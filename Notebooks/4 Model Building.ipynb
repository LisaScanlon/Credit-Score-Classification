{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa29ba36",
   "metadata": {},
   "source": [
    "# Model Building Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6750f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open('train_test_data.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13deb01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisal\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf9e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the unncessary columns from the input features for the models by dropping them now\n",
    "columns_to_exclude = ['ID', 'Customer_ID', 'Month', 'SSN', 'Type_of_Loan', 'Name']  # List of columns to exclude\n",
    "X_train = X_train.drop(columns=columns_to_exclude)\n",
    "X_test = X_test.drop(columns=columns_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd4d1b",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa691b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# Ran this code one to install, now it is being commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94ea6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Accuracy: 0.7723200494284832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Instantiate XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Train XGBoost classifier using the encoded target variables\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "print(\"XGBoost Classifier Accuracy:\", accuracy_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576dfcd5",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1a7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a1ea1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_lstm = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)))\n",
    "model_lstm.add(LSTM(units=50))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile LSTM model\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model\n",
    "model_lstm.fit(X_train_lstm, y_train, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test))\n",
    "\n",
    "# Evaluate LSTM model\n",
    "loss, accuracy = model_lstm.evaluate(X_test_lstm, y_test)\n",
    "print(\"LSTM Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb02bc0",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f01269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions using logistic regression model\n",
    "logistic_regression_preds = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Evaluate the logistic regression model\n",
    "logistic_regression_accuracy = logistic_regression_model.score(X_test, y_test)\n",
    "print(\"Logistic Regression Accuracy:\", logistic_regression_accuracy)\n",
    "\n",
    "# All of the other models follow the same basic procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd7f1f",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion = 'entropy', n_estimators = 500, max_depth = 5, random_state = 101)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "rf_score = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febf63e",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Machine (XGBoost)\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred_gbm = gbm.predict(X_test)\n",
    "gbm_score = accuracy_score(y_test, y_pred_gbm)\n",
    "print(\"GBM Accuracy:\", gbm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923e88d",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one takes a while to run\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "svm_score = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a736ff",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94417d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "knn_score = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"KNN Accuracy:\", knn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c8287",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "nb_score = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Naive Bayes Accuracy:\", nb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110aa8e",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "mlp_score = accuracy_score(y_test, y_pred_mlp)\n",
    "print(\"Neural Network Accuracy:\", mlp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947773ce",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_score = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy:\", dt_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643521ed",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19265f",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Methods - Bagging\n",
    "bagging = BaggingClassifier()\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "bagging_score = accuracy_score(y_test, y_pred_bagging)\n",
    "print(\"Bagging Classifier Accuracy:\", bagging_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64218773",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Methods - AdaBoost\n",
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost.predict(X_test)\n",
    "adaboost_score = accuracy_score(y_test, y_pred_adaboost)\n",
    "print(\"AdaBoost Classifier Accuracy:\", adaboost_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb026c",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Methods - Stacking (running this one takes a long time)\n",
    "estimators = [('dt', DecisionTreeClassifier()), ('bagging', BaggingClassifier()), ('gbm', GradientBoostingClassifier())]\n",
    "stacking = StackingClassifier(estimators=estimators)\n",
    "stacking.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking.predict(X_test)\n",
    "stacking_score = accuracy_score(y_test, y_pred_stacking)\n",
    "print(\"Stacking Classifier Accuracy:\", stacking_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8c5ee",
   "metadata": {},
   "source": [
    "Although the Stacking ensemble method had a slightly higher accuracy, we will use Bagging as our final model because the computation time is significantly faster, and the accuracy is only about 0.003 worse than the stacking model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c513bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
