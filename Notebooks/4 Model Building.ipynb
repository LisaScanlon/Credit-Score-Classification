{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa29ba36",
   "metadata": {},
   "source": [
    "# Model Building Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6750f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open('train_test_data.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13deb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf9e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the unncessary columns from the input features for the models by dropping them now\n",
    "columns_to_exclude = ['ID', 'Customer_ID', 'Month', 'SSN', 'Type_of_Loan', 'Name']  # List of columns to exclude\n",
    "X_train = X_train.drop(columns=columns_to_exclude)\n",
    "X_test = X_test.drop(columns=columns_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd4d1b",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa691b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# Ran this code one to install, now it is being commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94ea6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Accuracy: 0.7519308001235712\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Instantiate XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Train XGBoost classifier using the encoded target variables\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "print(\"XGBoost Classifier Accuracy:\", accuracy_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81857de2",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5695d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.7966481309854804\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Methods - Bagging\n",
    "bagging = BaggingClassifier()\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "bagging_score = accuracy_score(y_test, y_pred_bagging)\n",
    "print(\"Bagging Classifier Accuracy:\", bagging_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb02bc0",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f01269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6581711461229534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions using logistic regression model\n",
    "logistic_regression_preds = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Evaluate the logistic regression model\n",
    "logistic_regression_accuracy = logistic_regression_model.score(X_test, y_test)\n",
    "print(\"Logistic Regression Accuracy:\", logistic_regression_accuracy)\n",
    "\n",
    "# All of the other models follow the same basic procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd7f1f",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1789e0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 0.6762434352795799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion = 'entropy', n_estimators = 500, max_depth = 5, random_state = 101)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "rf_score = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Classifier Accuracy:\", rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febf63e",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ca0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Accuracy: 0.6984090206981773\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machine \n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred_gbm = gbm.predict(X_test)\n",
    "gbm_score = accuracy_score(y_test, y_pred_gbm)\n",
    "print(\"GBM Accuracy:\", gbm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c8287",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c7c6995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.5841056533827618\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "nb_score = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Naive Bayes Accuracy:\", nb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110aa8e",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45cf8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.6981773246833488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisal\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "mlp_score = accuracy_score(y_test, y_pred_mlp)\n",
    "print(\"Neural Network Accuracy:\", mlp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947773ce",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e709106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.72505406240346\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_score = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy:\", dt_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64218773",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6933f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Accuracy: 0.6719956750077232\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Methods - AdaBoost\n",
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost.predict(X_test)\n",
    "adaboost_score = accuracy_score(y_test, y_pred_adaboost)\n",
    "print(\"AdaBoost Classifier Accuracy:\", adaboost_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a24240",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a746fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.6677479147358666\n"
     ]
    }
   ],
   "source": [
    "# This one takes a while to run\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "svm_score = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb026c",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d79143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.7763361136855113\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Methods - Stacking (running this one takes a long time)\n",
    "estimators = [('dt', DecisionTreeClassifier()), ('bagging', BaggingClassifier()), ('gbm', GradientBoostingClassifier())]\n",
    "stacking = StackingClassifier(estimators=estimators)\n",
    "stacking.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking.predict(X_test)\n",
    "stacking_score = accuracy_score(y_test, y_pred_stacking)\n",
    "print(\"Stacking Classifier Accuracy:\", stacking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "455a6856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model Name  Accuracy Score\n",
      "1            Bagging        0.796648\n",
      "9           Stacking        0.776336\n",
      "0           XG Boost        0.751931\n",
      "6      Decision Tree        0.725054\n",
      "3  Gradient Boosting        0.698409\n",
      "5     Neural Network        0.698177\n",
      "2      Random Forest        0.676243\n",
      "7           Adaboost        0.671996\n",
      "8                SVM        0.667748\n",
      "4        Naive Bayes        0.584106\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define evaluation data\n",
    "evaluation_data = [(\"XG Boost\", accuracy_xgb), (\"Bagging\", bagging_score), \n",
    "                   (\"Random Forest\", rf_score), (\"Gradient Boosting\", gbm_score),\n",
    "                   (\"Naive Bayes\", nb_score),\n",
    "                   (\"Neural Network\", mlp_score), (\"Decision Tree\", dt_score),\n",
    "                   (\"Adaboost\", adaboost_score), (\"SVM\", svm_score), (\"Stacking\", stacking_score)]\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_data, columns=[\"Model Name\", \"Accuracy Score\"])\n",
    "\n",
    "# Sort DataFrame by 'R2 Score' column in descending order\n",
    "evaluation_df = evaluation_df.sort_values(by=\"Accuracy Score\", ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(evaluation_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8c5ee",
   "metadata": {},
   "source": [
    "Although the Stacking ensemble method had a slightly higher accuracy, we will use Bagging as our final model because the computation time is significantly faster, and the accuracy is only about 0.003 worse than the stacking model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98179aa5",
   "metadata": {},
   "source": [
    "## Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37bb2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG Boost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.58      0.85      0.69      2100\n",
      "        Poor       0.74      0.83      0.78      4018\n",
      "    Standard       0.86      0.68      0.76      6830\n",
      "\n",
      "    accuracy                           0.75     12948\n",
      "   macro avg       0.73      0.78      0.74     12948\n",
      "weighted avg       0.78      0.75      0.75     12948\n",
      "\n",
      "Bagging:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.71      0.82      0.76      2100\n",
      "        Poor       0.77      0.86      0.81      4018\n",
      "    Standard       0.85      0.75      0.80      6830\n",
      "\n",
      "    accuracy                           0.80     12948\n",
      "   macro avg       0.78      0.81      0.79     12948\n",
      "weighted avg       0.80      0.80      0.80     12948\n",
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.49      0.81      0.61      2100\n",
      "        Poor       0.64      0.70      0.67      4018\n",
      "    Standard       0.80      0.59      0.68      6830\n",
      "\n",
      "    accuracy                           0.66     12948\n",
      "   macro avg       0.64      0.70      0.65     12948\n",
      "weighted avg       0.70      0.66      0.66     12948\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.49      0.82      0.61      2100\n",
      "        Poor       0.64      0.79      0.71      4018\n",
      "    Standard       0.86      0.56      0.68      6830\n",
      "\n",
      "    accuracy                           0.68     12948\n",
      "   macro avg       0.66      0.72      0.67     12948\n",
      "weighted avg       0.73      0.68      0.68     12948\n",
      "\n",
      "Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.51      0.84      0.64      2100\n",
      "        Poor       0.68      0.80      0.73      4018\n",
      "    Standard       0.85      0.60      0.70      6830\n",
      "\n",
      "    accuracy                           0.70     12948\n",
      "   macro avg       0.68      0.74      0.69     12948\n",
      "weighted avg       0.74      0.70      0.70     12948\n",
      "\n",
      "Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.40      0.86      0.55      2100\n",
      "        Poor       0.59      0.79      0.67      4018\n",
      "    Standard       0.84      0.38      0.52      6830\n",
      "\n",
      "    accuracy                           0.58     12948\n",
      "   macro avg       0.61      0.67      0.58     12948\n",
      "weighted avg       0.69      0.58      0.57     12948\n",
      "\n",
      "Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.51      0.83      0.63      2100\n",
      "        Poor       0.71      0.75      0.73      4018\n",
      "    Standard       0.81      0.63      0.71      6830\n",
      "\n",
      "    accuracy                           0.70     12948\n",
      "   macro avg       0.68      0.74      0.69     12948\n",
      "weighted avg       0.73      0.70      0.70     12948\n",
      "\n",
      "Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.67      0.66      0.66      2100\n",
      "        Poor       0.72      0.70      0.71      4018\n",
      "    Standard       0.74      0.76      0.75      6830\n",
      "\n",
      "    accuracy                           0.73     12948\n",
      "   macro avg       0.71      0.71      0.71     12948\n",
      "weighted avg       0.72      0.73      0.72     12948\n",
      "\n",
      "Adaboost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.49      0.83      0.62      2100\n",
      "        Poor       0.65      0.74      0.69      4018\n",
      "    Standard       0.82      0.58      0.68      6830\n",
      "\n",
      "    accuracy                           0.67     12948\n",
      "   macro avg       0.66      0.72      0.66     12948\n",
      "weighted avg       0.72      0.67      0.68     12948\n",
      "\n",
      "SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.48      0.81      0.61      2100\n",
      "        Poor       0.64      0.76      0.69      4018\n",
      "    Standard       0.83      0.57      0.68      6830\n",
      "\n",
      "    accuracy                           0.67     12948\n",
      "   macro avg       0.65      0.71      0.66     12948\n",
      "weighted avg       0.72      0.67      0.67     12948\n",
      "\n",
      "Stacking:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.77      0.64      0.70      2100\n",
      "        Poor       0.78      0.78      0.78      4018\n",
      "    Standard       0.77      0.82      0.79      6830\n",
      "\n",
      "    accuracy                           0.78     12948\n",
      "   macro avg       0.78      0.75      0.76     12948\n",
      "weighted avg       0.78      0.78      0.77     12948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification reports of each of the models\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Transform predicted labels back to original classes just for XG Boost\n",
    "y_pred_xgb_original = label_encoder.inverse_transform(y_pred_xgb)\n",
    "\n",
    "print(\"XG Boost:\")\n",
    "print(classification_report(y_test, y_pred_xgb_original))\n",
    "\n",
    "print(\"Bagging:\")\n",
    "print(classification_report(y_test, y_pred_bagging))\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_test, logistic_regression_preds))\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Gradient Boosting:\")\n",
    "print(classification_report(y_test, y_pred_gbm))\n",
    "\n",
    "print(\"Naive Bayes:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "print(\"Neural Network:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "print(\"Decision Tree:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "print(\"Adaboost:\")\n",
    "print(classification_report(y_test, y_pred_adaboost))\n",
    "\n",
    "print(\"SVM:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "print(\"Stacking:\")\n",
    "print(classification_report(y_test, y_pred_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a948ea",
   "metadata": {},
   "source": [
    "The F1 scores generated in the classification report also show that XG Boost and Bagging are the best models. Based on both the accuracy scores and the classification reports, both the XG Boost model and the Bagging model will move on to the parameter tuning and complex model design stage. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
